
Notes on the fly: Math Camp for 9.520/6.860S Statistical Learning Theory and Applications
https://www.youtube.com/watch?v=AsogCoscZgE&list=PL_Ig1a5kxu55ivmyrfRmeUOFeaaWuqPpg

We like ℝᴰ because we can 
* add elements
* multiply by numbers 3v
* take inner product ... this is the "richest situation" can get the other stuff
* then norms  can define as Sqrt[ (Sum[ v[[j]] ])^2 ]
* then distances 

We want to do the same thing with D = Infinity

### vector space

+: V x V -> V and ℝ x V -> V

1. v + w = w + v

...


ℝ^n, space of polynomials, functions, etc

#### inner product (scalar product)

<·,·> V x V -> ℝ

it takes two objects and returns a number

v,w member of V are orthogonal if <v,w> = 0

Cauchy-Schwarz inequality <v,w> gte <v,v>^1/2 <w,w>^1/2

Can define norm from inner product ||v||=<v,v>^1/2

#### norm 

is a function ||·||: V -> ℝ





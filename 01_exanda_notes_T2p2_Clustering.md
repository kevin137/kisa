# T2.2 Clustering

Unsupervised classification, clustering

## Introduction

Let Î© = {1,2, â€¦, _n_} be a set of _n_ objects over which 
different variables have been measured.

## Principal objective

* **Find the various possible groups or clusters that make
  up these clusters**
* The objects grouped into the same cluster should be
  similar among themselves and different to the objects
  grouped into the other clusters.
* The clusters are created without previous information
  and are suggested by the "essence" of the data themselves.

## Applications

* Microarray data: group and identify genes that act in
  similar or different way in biological processes
* Segmentation of images: identify the diferent elements
  present in an image.
* etc...

## What is a cluster?

In some cases, it's not easy to determine.

### Overall division of techniques

En general, clustering techiques are divided into two 
large groups:

* Hierarchical methods

  Belonging to a cluster at a given level of hierarchy
  determines the membership to clusters at a higher
  level

  - Agglomerative
  - Divisive

  Built starting with a distance matrix

* Partitional methods

  Provide a unique partitioning, one the number of
  clusters _k_ has been set.

  Built starting with a matrix of data

* Other techniques: Models based on densities,
  clustering, ...

## Hierarchical methods

### What is a hierarchy?

> Hierarchical methods construct indexed hierarchies.

Suppose that we have _n_ = 5 objects collected in 
Î© = {1,2,3,4,5}. The following sequence of partitiones 
establish a hierachy:

>          Pâ‚€ = {{1}, {2}, {3}, {4}, {5}}
>          Pâ‚ = {{1}, {2}, {3}, {4, 5}}
>          Pâ‚‚ = {{1}, {2, 3}, {4, 5}}
>          Pâ‚ƒ = {{1, 2, 3}, {4, 5}}
>          Pâ‚„ = {{1, 2, 3, 4, 5}} = {Î©}

### Agglomerative vs. devisive

Depending on the order we move through the partitions 
defined before:

* P0 â†’ P4: Agglomerative
  In each step the new clusters are created by uniting
  clusters from the previous step
  - Fast

* P4 â†’ P0: Divisive
  In each step the new clusters are created by dividing
  clusters from the previous step 
  - Based on the global information Parten de la informaciÂ´on global de los datos.
  - Slow

### Dendrogram

Hierarchical classifications con be represented on a 
bidirectional diagram called a **dendrogram**

![Dendrogram](https://upload.wikimedia.org/wikipedia/commons/c/ce/Dendrogram2.png)
<!--- Jakub al13, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, 
        via Wikimedia Commons -->

### Indexed hierarchy

An indexed hierarchy (ð‚, Î±) over a collection of objects Î© 
forms a collection of clusters ð‚ âŠ‚ P(Î©) and an index
Î± : ð‚ â†’ â„âº such that ð‚ complies with the following axioms:

  1. Intersection axiom: if C,Câ€² âˆˆ ð‚, then 
     C âˆ© Câ€² âˆˆ {C, Câ€², âˆ…},
  2. Union axiom: if C âˆˆ ð‚, then 
     C = â‹ƒ{Câ€² | Câ€² âˆˆ ð‚ and Câ€² âŠ‚ C},
  3. The union of all the clusters contain all the cases:
     Î© = â‹ƒ{C | C âˆˆ ð‚};
     
And the index Î± obeys:
* Î±(i) = 0, âˆ€i âˆˆ Î©,
* Î±(C) â‰¤ Î±(Câ€²) if C âŠ‚ Câ€².

### Where are the clusters?

What are the partitions that a dendrogram or indexed hierarchy 
provides us?

âŸ¶ To see them, you must **cut the dendrogram**

### Ultrametric space

We say that (Î©,u) is an ultrametric space if _u_ is a 
distance funcion defined over Î©xÎ© and additionally 
obeys the _ultrametric property_. Which is:

>          u : Î© Ã— Î© â†¦ â„
>              (i,j) â†¦ u(i,j) = uáµ¢â±¼

Such that:
1. u(i,j) â‰¥ 0 âˆ€i, j âˆˆ Î©
2. u(i,i) = 0 âˆ€i âˆˆ Î©
3. u(i,j) = u(j,i) âˆ€i, j âˆˆ Î©
4. Ultrametric property
   u(i,j) â‰¤ supâ‚–{u(i,k),u(j,k)}, âˆ€i, j âˆˆ Î©

#### All triangles are isosceles?

Property
> In an ultrametric space, all triangles are isosceles.

#### Ultrametric triangle:
An isosceles triangle with the base is the smallest side.

Example:

>              0   0.2  0.2   0.35  0.35
>                   0   0.15  0.35  0.35
>          U =           0    0.35  0.35
>                              0    0.1
>                                    0






2023-09-19 EXANDA lecture starts on slide "MÃ©todos particionales"

next slide "Datos iniciales - ejemplo"

C1 = { 3,4,5 }, C2 = { 1,3 }

Why is this partition worse?

> The centroid of everything is too close to the centroid of one of the clusters and, and other problems

### Conceptos basicos

T, W, B 

Total variability = variability within clusters +  variability between clusters

#### Criterios

Idea intuitiva:
* Find partitions with small W
* Find partitions with large B

1. Minimize the trace of W
2. Minimize the determinant of W
3. Minimize the Â¿coefficient? det(W)/det(B)
4. Minimize the trace of (W^-1)B

Remember ---> Trace of a matrix tr(W) is just the sum of the diagonals, w1,1+w2,2+w3,3+...wn,n

skipping some slide to "Algoritmos iteratives de recolocaciÃ³n"

1,3 semillas

C1={1,2}, C2={3,4,5}

Did we improve the situation?

### K-means

now K-medias (slide)

New slide with R code and my scribblings "Ejemplo - Resultados de R"

try various differnt nstart

Table 1

Table 2 

etc, fill in later

#### A tener in cuenta

* only indicated for cuantative variables Euclidean distances only
*  results depend on seeds, try different seeds
*  No stability in the partitions obtained
  - The number of fixed clusters k is not adequate
  - The is no structure to the cluster

### PAM (Partitioning Around Medoids)

PAM is very commonly used: No need for original data, with the distances you have enough

#### A tener en cuenta

* Applicable to a distance matrix
* Robust against outliers
* Puede ser no adecuado para conjuntos de datos muy grandes (-->CLARA)

Slide of R code

PAM is in library(cluster)

New slide Â¿Cuantos cluster hay?

diagram sketch with 3 clusters

### Silhouettes

a few more slides, now 

* s(i) nos da la 'calidad' de la clasificaciÃ³n de i.
* Anchura media de silhouette "calidad" para un cluster C --> individual cluster
* Anchura media global de silhouette "calidad" de los datos ---> the whole organization of clusters

next slide "Ejemplo - Ruspini data"  ---> famous data

many more slides.... now "Aproximaciones para la validaciÃ³n"

Ill-posed-problem... but it can have a valid interpretation

1. INternal validation: coheision, stability
   * Sometimes Silhouette can be considered here
   * Dunn index
   * Tecnicas Monte Carlo para testear H0: 'No hay structura'
2. External validation:
   Hay una partition P={P1,...Ps} of external reference
   * Rand and Hubert indices

Interesting lecture: Henning, C. (2015) what are true clusterss= Pattern Recognition Letters, 64: 53.62

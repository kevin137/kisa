## Notes from 9.520/6.860: Statistical Learning Theory and Applications - Class 3
https://www.youtube.com/live/MiypgGqEPpQ?si=IesOs32QuPPaVSZo
> Prof. Lorenzo Rosasco, University of Genoa / MIT
> 
### Empirical Risk Minimization (with the Least Squares loss function )

Known by many names, M estimation in statistics

One of the most studied prinicples to develop algorithms
in machine learning. Probably most classical and simple
instantiation, interesting because despite looking at
super complicated nonlinear models like neural networks, 
when you want to try to understand what's going on you 
often actually go back to this. Surprising number of 
papers in the last 12 months on linear least squares
attempting to explain things you see in much more 
complicated models.

#### Initial definitions

>                        ___L(f)___
>           min    ğ”¼â‚“,áµ§[ â„“(y,f(x)) ]
>          f: xâ†’y
>
>          given Sâ‚™ = (xáµ¢,yáµ¢)â‚â¿  ~ Pâ¿

With Sâ‚™ being the training set which is a finite set of 
input-output pairs (which is approximating a perfect 
probabilistic expression of the population statistics,
my words...)

(The expectation value in the expression above is 
trying to say that prediction for more common events is 
more important for less common... so its like a weighted sum)

Side remark, this is assuming i.i.d. 
"Independent and identically distributed random variables". 
This requirement can be relaxed in some cases. 
Assuming that the data all come from the same distribution 
is much more crucial. The distribution they are coming from 
actually _defines_ the objective.
(Sidetrack on sequential predection where this is not true)

There is no way to do this exactly, we will have to 
approximate it.

#### What is an algorithm ?

A map that given the data is going to return some function

>          Sâ‚™ âŸ¼ ^f = ^f_Sâ‚™ â‰ˆ fâ‚š
>          L(^f) â‰ˆ L(fâ‚š)

( ^f  as in "f hat"; hat means an empirical quantity, a 
quantity that _depends on the data_, both the 
cardinality and the specific instance. fâ‚š is the 
theoretical function that would be the exact minimizer
of these overall possible functions )

We would like a solution whose test error is close to the 
best possible test error. 

There is no solution, for at least two reasons:

1. There is no expectation, the computation of it extremely
   hard or impossible
2. The function space is too big

_Gradient descent might be a possible algorithm, if we knew
some more things_

#### Hypothesis space

>          (1)    ğ“— âŠ‚ { f | X âŸ¶ â„ }

( we are keeping to the reals for now, in other words, a 
space for which the expectation is well-defined )

Let's look at the space of possible solutions, the 
hypothesis space, and now let's look inside for a 
smaller space

in here we will replace â„“(f) with empirical risk

>          (2)    ^L(f) = (1/n)Â·( âˆ‘i:1:n â„“(yáµ¢,f(xáµ¢)) ) 

so, we are looking for this function f() to minimize the
loss function

>                   min   (1/n)Â·( âˆ‘i:1:n â„“(yáµ¢,f(xáµ¢)) ) 
>                  f âˆˆ H

We are in good shape, we have access to everthing, the data 
the loss function and the space. There is a tension, we want 
H to be something manageable, yet as big as possible to not
restrict the search for solutions.

**important point** Rather than replacing the objective function 
with another empirical function you can compute from the data, the 
other typical approach is to replace the gradient of that function 
with an empirical approximation. Can also do things like the
stochastic gradient, where you take the gradient of just one point. 

We will describe the two different approaches, replacing 
the gradient or replacing the objective function; they are both ways
to explore the possible space of solutions. Things like optimization 
and projection might guide the search.

To continue we will make some specfic choices, 

First let's take the hypothesis space H to be linear functions ...

H : { f:Xâ†’â„ | f(x) = wáµ€x }  X: â„áµˆ

... there exists a w for which I can do this for every x 

once I do this, I can replace the idea of function, 
which is a very long list of association of numbers,
with vectors, which is a list of d numbers. 
This is simple, but this is the foundation of any other 
model: feature maps, kernel, neural networks

We will eventually replace the â„“ with the square loss function

Now we can start identifying H with â„áµˆ
> H â‰ƒ â„áµˆ

So now we can talk about things like what is the norm 
of the function in H:
> âˆ¥fâˆ¥_H = âˆ¥wâˆ¥

we can define an inner product:
> âŸ¨f,f'âŸ©_H = wáµ€w'

(Note: they sometimes use the âŸ¨âŸ© notation in the
 course to denote infinite objects)

We love this model because instead of thinking
about the functions you can think about
of vectors and do all the stuff you do for vectors.
It's clearly a linear space,
it clearly has an inner product,
a norm, a distance, everything you want.
You can talk about orthogonality and basically
every problem we're going to discuss is going to
become essentially a linear algebra or calculus
problem.

With this choice in mind the least squares problem 
(using square loss function) becomes the following 
minimization:

>              min   (1/n)Â·( âˆ‘i:1:n (yáµ¢ - wáµ€xáµ¢)Â² ) 
>             w âˆˆ â„áµˆ

We don't introduce the offset, it's very easy to plug it
back in and in high dimension, it really doesn't matter 
too much.

This problem is very related to linear algebra, so it's 
very useful to rewrite in linear algebra notation:

>          ^Xâ‚™â‚“áµˆ = ( [x rowáµˆ]â‚ â€¦ [x rowáµˆ]â‚™ )áµ€ 
>          ^Yâ‚™â‚“â‚ = ( yâ‚ â€¦ yâ‚™ )áµ€ 

Now we can rewrite the problem as the minimziation of 
this quadratic residual:

>              min   (1/n)Â·( âˆ‘i:1:n âˆ¥^XÂ·w - ^Y)âˆ¥Â² ) 
>             w âˆˆ â„áµˆ

The story until now was a learning-related story, now it
could go in a couple of directions:
1. Prove using statistical/probabilistic tools that it is valid
2. What is this from a computational point of view?

For the moment we can forget about the statistical viewpoint...
this is now a linear algebra problem, particularly a...

#### Least squares problem associated with a linear system

>          ^XÂ·w = ^Y

Linear systems can be in two situations:

##### Linear system situation 1: n > d  
   (over-determined linear system of equations) 
   from a machine learning point of view, an 
   under-parameterized model, the number of parameters 
   is smaller than the number of datapoints
   (here we assuming that the columns are linearly
    independent, or d columns are full rank)

![n > d](/SLTA_notes_Class03_n_gt_d.png)
>             âˆ„^w s.t. (such that) ^XÂ·w = ^Y


How do we solve this system? We take the gradient of expression 
from before
>             (1/n)Â·( âˆ¥^XÂ·w - ^Y)âˆ¥Â² ) 

set it equal to zero, and solve

>             âˆ‡( (1/n)Â·( âˆ‘i=1:n âˆ¥^XÂ·w - ^Y)âˆ¥Â² ) = 0 
>
>             2Â·^Xáµ€(^Xw - ^Y) = 0
>
>             ^Xáµ€Â·^XÂ·w = ^Xáµ€Â·^Y
>                                note that ^Xáµ€Â·^X is dxd,
>                                so we can invert it 

###### Solution of w for n > d

>             w = (^Xáµ€Â·^X)â»Â¹Â·^Xáµ€Â·^Y

because (the outermost operator, outside of the norm) is just a 
square I'm just gonna get optimality conditions that are linear 
and I get a linear system I solve this linear system, and that's
it.

##### Linear system situation 2: n < d

Consider now the situation where n is much smaller than d

![n < d](/SLTA_notes_Class03_n_lt_d.png)
>             âˆƒ^w s.t. (such that) ^XÂ·w = ^Y

The space of variables is actually larger than the space of data,
so I can see the space of data as a subset of the larger space â„áµˆ.
Assuming now n is smaller than d and n is full rank (the rows are 
linearly independent) 

At this point I don't have the problem that a solution might not
exist, it does exist. What might happen is that multiple solutions 
might exist. So now I need to have a selection criteria, and the 
classical one is: consider among all the possible solutions to the
system, the one with the **minimum norm**.

>               min   âˆ¥wâˆ¥Â² subject to  ^Xw = ^Y
>              w âˆˆ â„áµˆ

**important** this will around multiple times during this course

How do we find the solution in this case? It is more annoying 
because we can't just take the gradient and set to zero...

Set up Lagrange multipliers, set ^XÂ·w = ^y up as a constraint and
set the optimality? of that equal to zero.

>              min âˆ¥wâˆ¥Â² + Î»Â·Î±áµ€(^Xw - ^Y)

New objective function:

>              âˆ¥wâˆ¥Â² + Î±áµ€(^Xw - ^Y)

Working through this, we get:

###### Solution of w for n < d

>              ^w = ^Xáµ€(^XÂ·^Xáµ€)â»Â¹Â·^Y

Again we are assuming that the rows are linearly independent,
so that when we build the ^XÂ·^Xáµ€ this matrix is now n by n so we 
can assume it is invertible and we get this previous equation.

Now some insider baseball in response to a question about
different kinds of norms...
What we just did is the simple example because it's the square,
you take derivative and it's nice, but say in the last 10 years 
people, I guess from 2000 to 2010, people were obsessed with the 
L1 norm. You can use entropy here you can use a lot of different 
things and you can use things that are not norms you can use
functions that you like. This is the place where, if you like
the loss function, you have to make a design choice, and and what 
we did before is a design choice and we're gonna call this a prior 
and we're gonna see that this actually makes a lot of difference 
depending what you put here.

The thing that we just rediscovered is called the "pseudoinverse".

#### Pseudoinverse of the least squares problem

_Moore-Penrose Pseudoinverse_ 
[wikipedia](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)

>          ^w = (^Xâ€ )^Y
> 
>          ^Xâ€  = (^Xáµ€Â·^X)â»Â¹Â·^Xáµ€    for n > d (independent columns)
> 
>          ^Xâ€  = ^Xáµ€(^XÂ·^Xáµ€)â»Â¹     for n < d (independent rows)

_Note: the notation is getting squirelly, so here's the same 
thing in LaTeX for clarity:_

  ```math
    \begin{multline}
      \begin{split}
        \hat{w} &= \hat{X}^{\dagger}\hat{Y} \\
        \hat{X}^{\dagger} &= (\hat{X}^{T}\hat{X})^{-1}\hat{X}^{T} 
        \hspace{12pt} \text{ for n > d, (independent columns) } \\ 
        \hat{X}^{\dagger} &= \hat{X}^{T}(\hat{X}\hat{X}^{T})^{-1}
        \hspace{12pt} \text{ for n < d, (independent rows) } \\ 
      \end{split}
    \end{multline}
  ```
Observations:
1. If you have a rectangular matrix, computationally what you
   do for the pseudoinverse, is check which (^Xáµ€Â·^X) is the
   smallest of the two, invert that matrix and solve the problem  
2. The computational cost of inverting the matrix to calculated
   the n > d case, is the size of the matrix cubed, plus some
   other stuff, so a total cost of dÂ³ + ndÂ² ,
   for the n > d case, it is nÂ³ + dÂ²n
3. Starting with ^w = ^Xáµ€(^XÂ·^Xáµ€)â»Â¹Â·^Y, what is the
   size/dimension of the "(^XÂ·^Xáµ€)â»Â¹Â·^Y" part? It is an
   n-dimenstional vector, so we will give it a name: C

>          ^w = ^Xáµ€(^XÂ·^Xáµ€)â»Â¹Â·^Y
>             = ^Xáµ€Â·C
>             = âˆ‘_i:1:n xáµ¢Â·cáµ¢   âˆˆ NullSpace(^Xáµ€)

The "w" are linear combinations of the input power, we will 
call this the "representor theorem", and it is the way to build 
non-parametric models through kernels.
          
>          ^fâ€ (x) = xáµ€Â·^wâ€ 
>                 = âˆ‘_i:1:n xáµ¢áµ€Â·xáµ¢Â·cáµ¢ 

Can view this as a linear combination or a kind of correlation 
between the new input and the input points, think of this 
inner product as some kind of similarity, not really because 
they're not normalized, but a kind of the angle related to the 
angle between my vectors. In other words, a weighted combination 
of the similarites where here the similarity is just the inner
product.

What do these solutions actually look like? 

>          ^XÂ·w = ^Y

Anything in the null space of X is going to be another solution...

>          ^wâ€  + wâ‚€   wâ‚€ âˆˆ NullSpace(^X)

_Null Space_ [wikipedia](https://en.wikipedia.org/wiki/Kernel_(linear_algebra))

Remember you can decompose the space in the null space of X 
and the range of Xáµ€, that's a basic theorem in calculus. 
This shows that ^wâ€  belongs to the range of Xáµ€â€  

Another point, we chose the only vector(s) orthogonal to the 
null space of the matrix. So, you can think in terms of minimum norm,
or you can think in terms of your tunnel to this space, it's all the 
same and again this is all just big recap of linear systems because 
we're actually going to build quite a bit on this in all sorts of ways

#### Single Value Decomposition (SVD) of the matrix X

[wikipedia](https://en.wikipedia.org/wiki/Singular_value_decomposition)

>          ^X = UÂ·Î£Â·Váµ€

----- taking a break at 48:25










